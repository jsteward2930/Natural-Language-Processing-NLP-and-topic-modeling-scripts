{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# First cell: Install required packages\n",
        "!pip install gensim nltk spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p070OaoSKNIP",
        "outputId": "eb1761d6-779d-4381-d524-ea38a0ee5f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second cell: Import libraries and download ALL required NLTK data\n",
        "import numpy as np\n",
        "from gensim import corpora, models\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "import spacy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download ALL required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')  # Open Multilingual Wordnet\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Verify the downloads\n",
        "print(\"Verifying NLTK downloads...\")\n",
        "try:\n",
        "    # Test tokenization\n",
        "    test_text = \"This is a test sentence.\"\n",
        "    tokens = word_tokenize(test_text)\n",
        "    print(\"Tokenization test successful!\")\n",
        "\n",
        "    # Test stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    print(\"Stopwords loaded successfully!\")\n",
        "\n",
        "    # Test lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    test_word = lemmatizer.lemmatize(\"testing\")\n",
        "    print(\"Lemmatizer test successful!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during verification: {str(e)}\")\n",
        "    print(\"Please run the downloads again if you see any errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAHqiA8RQFS6",
        "outputId": "b98174a2-57c8-469c-b1c6-08c27964d87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying NLTK downloads...\n",
            "Tokenization test successful!\n",
            "Stopwords loaded successfully!\n",
            "Lemmatizer test successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third cell: Main analyzer class\n",
        "class AcademicParagraphAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the analyzer with necessary NLP components\"\"\"\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        # Add academic-specific stopwords\n",
        "        self.stop_words.update(['et', 'al', 'fig', 'figure', 'table', 'eq', 'equation'])\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess the input text for analysis\n",
        "\n",
        "        Args:\n",
        "            text (str): Input paragraph text\n",
        "\n",
        "        Returns:\n",
        "            list: Preprocessed tokens\n",
        "            str: Preprocessed text\n",
        "        \"\"\"\n",
        "        # Convert to lowercase and tokenize\n",
        "        tokens = word_tokenize(text.lower())\n",
        "\n",
        "        # Remove punctuation and numbers\n",
        "        tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "        # Remove stopwords and lemmatize\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens\n",
        "                 if token not in self.stop_words]\n",
        "\n",
        "        # Reconstruct processed text\n",
        "        processed_text = ' '.join(tokens)\n",
        "\n",
        "        return tokens, processed_text\n",
        "\n",
        "    def extract_topics_lda(self, text, num_topics=3, num_words=5):\n",
        "        \"\"\"\n",
        "        Extract topics using Latent Dirichlet Allocation\n",
        "\n",
        "        Args:\n",
        "            text (str): Input paragraph text\n",
        "            num_topics (int): Number of topics to extract\n",
        "            num_words (int): Number of words per topic\n",
        "\n",
        "        Returns:\n",
        "            list: List of topics with their key terms and weights\n",
        "        \"\"\"\n",
        "        tokens, _ = self.preprocess_text(text)\n",
        "\n",
        "        # Create dictionary and corpus\n",
        "        dictionary = corpora.Dictionary([tokens])\n",
        "        corpus = [dictionary.doc2bow(tokens)]\n",
        "\n",
        "        # Train LDA model\n",
        "        lda_model = models.LdaModel(\n",
        "            corpus,\n",
        "            num_topics=num_topics,\n",
        "            id2word=dictionary,\n",
        "            passes=15,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Extract topics\n",
        "        topics = []\n",
        "        for idx, topic in lda_model.show_topics(\n",
        "            num_topics=num_topics,\n",
        "            num_words=num_words,\n",
        "            formatted=False\n",
        "        ):\n",
        "            topic_terms = [(term, round(weight, 4)) for term, weight in topic]\n",
        "            topics.append(topic_terms)\n",
        "\n",
        "        return topics\n",
        "\n",
        "    def extract_key_phrases(self, text):\n",
        "        \"\"\"\n",
        "        Extract key phrases using spaCy's noun chunk extraction\n",
        "\n",
        "        Args:\n",
        "            text (str): Input paragraph text\n",
        "\n",
        "        Returns:\n",
        "            list: List of key phrases with their importance scores\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Extract noun phrases and calculate their importance\n",
        "        phrases = []\n",
        "        for chunk in doc.noun_chunks:\n",
        "            # Clean the phrase\n",
        "            phrase = ' '.join([token.text for token in chunk\n",
        "                             if token.text.lower() not in self.stop_words])\n",
        "            if phrase:\n",
        "                # Calculate importance score based on phrase length and position\n",
        "                importance = len(chunk) / len(doc)\n",
        "                phrases.append((phrase, round(importance, 4)))\n",
        "\n",
        "        # Sort by importance score\n",
        "        phrases.sort(key=lambda x: x[1], reverse=True)\n",
        "        return phrases[:5]  # Return top 5 phrases\n",
        "\n",
        "    def generate_summary(self, text):\n",
        "        \"\"\"\n",
        "        Generate a concise summary using TF-IDF and sentence scoring\n",
        "\n",
        "        Args:\n",
        "            text (str): Input paragraph text\n",
        "\n",
        "        Returns:\n",
        "            str: Generated summary\n",
        "        \"\"\"\n",
        "        # Split into sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "        if len(sentences) <= 2:\n",
        "            return text  # Return original text if too short\n",
        "\n",
        "        # Create TF-IDF matrix\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "        # Calculate sentence scores\n",
        "        sentence_scores = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            score = np.sum(tfidf_matrix[i].toarray())\n",
        "            sentence_scores.append((sentence, score))\n",
        "\n",
        "        # Sort sentences by score\n",
        "        sentence_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Select top sentences for summary\n",
        "        num_sentences = max(1, len(sentences) // 3)\n",
        "        summary_sentences = [score[0] for score in sentence_scores[:num_sentences]]\n",
        "\n",
        "        # Reconstruct summary in original order\n",
        "        summary = []\n",
        "        for sentence in sentences:\n",
        "            if sentence in summary_sentences:\n",
        "                summary.append(sentence)\n",
        "\n",
        "        return ' '.join(summary)\n",
        "\n",
        "    def analyze_paragraph(self, text):\n",
        "        \"\"\"\n",
        "        Perform comprehensive analysis of an academic paragraph\n",
        "\n",
        "        Args:\n",
        "            text (str): Input paragraph text\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results including topics, key phrases, and summary\n",
        "        \"\"\"\n",
        "        topics = self.extract_topics_lda(text)\n",
        "        key_phrases = self.extract_key_phrases(text)\n",
        "        summary = self.generate_summary(text)\n",
        "\n",
        "        analysis = {\n",
        "            'topics': topics,\n",
        "            'key_phrases': key_phrases,\n",
        "            'summary': summary\n",
        "        }\n",
        "\n",
        "        return analysis"
      ],
      "metadata": {
        "id": "GWsDDrrGQW1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fourth cell: Example usage and testing\n",
        "# Sample academic paragraph\n",
        "sample_text = \"\"\"\n",
        "Recent advances in deep learning have revolutionized natural language processing tasks.\n",
        "Transformer architectures, particularly BERT and its variants, have achieved state-of-the-art\n",
        "results in various benchmark datasets. These models leverage self-attention mechanisms to\n",
        "capture long-range dependencies in text, enabling better understanding of context and semantic\n",
        "relationships. Furthermore, pre-training on large corpora followed by task-specific fine-tuning\n",
        "has proven to be an effective transfer learning approach.\n",
        "\"\"\"\n",
        "\n",
        "# Create analyzer instance\n",
        "analyzer = AcademicParagraphAnalyzer()\n",
        "\n",
        "# Analyze paragraph\n",
        "results = analyzer.analyze_paragraph(sample_text)\n",
        "\n",
        "# Print results\n",
        "print(\"Topics:\")\n",
        "for i, topic in enumerate(results['topics']):\n",
        "    print(f\"Topic {i+1}:\", topic)\n",
        "\n",
        "print(\"\\nKey Phrases:\")\n",
        "for phrase, score in results['key_phrases']:\n",
        "    print(f\"- {phrase} (score: {score})\")\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "print(results['summary'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bldk80sgQhun",
        "outputId": "01fc480f-23bd-4c91-8bb5-ad592d88e4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('learning', 0.0257), ('benchmark', 0.0256), ('dependency', 0.0256), ('achieved', 0.0256), ('context', 0.0256)]\n",
            "Topic 2: [('learning', 0.044), ('approach', 0.0252), ('revolutionized', 0.0252), ('task', 0.0252), ('better', 0.0252)]\n",
            "Topic 3: [('learning', 0.0257), ('mechanism', 0.0257), ('large', 0.0257), ('recent', 0.0257), ('understanding', 0.0257)]\n",
            "\n",
            "Key Phrases:\n",
            "- task - specific fine - tuning (score: 0.0638)\n",
            "- effective transfer learning approach (score: 0.0532)\n",
            "- natural language processing tasks (score: 0.0426)\n",
            "- self - attention mechanisms (score: 0.0426)\n",
            "- long - range dependencies (score: 0.0426)\n",
            "\n",
            "Summary:\n",
            "These models leverage self-attention mechanisms to \n",
            "capture long-range dependencies in text, enabling better understanding of context and semantic \n",
            "relationships.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fifth cell: Function for analyzing new paragraphs\n",
        "def analyze_new_paragraph(text):\n",
        "    \"\"\"\n",
        "    Wrapper function to easily analyze new paragraphs\n",
        "\n",
        "    Args:\n",
        "        text (str): The paragraph to analyze\n",
        "    \"\"\"\n",
        "    results = analyzer.analyze_paragraph(text)\n",
        "\n",
        "    print(\"Topics:\")\n",
        "    for i, topic in enumerate(results['topics']):\n",
        "        print(f\"Topic {i+1}:\", topic)\n",
        "\n",
        "    print(\"\\nKey Phrases:\")\n",
        "    for phrase, score in results['key_phrases']:\n",
        "        print(f\"- {phrase} (score: {score})\")\n",
        "\n",
        "    print(\"\\nSummary:\")\n",
        "    print(results['summary'])"
      ],
      "metadata": {
        "id": "Za0sn-_0Q97N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"Your paragraph text here\")"
      ],
      "metadata": {
        "id": "XGKpSyPB_801"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"Machine learning models have several applications in the implementation of genomic medicine such as to recommend diagnostic tools and pharmacogenomic therapies based on the patient’s genetic makeup.15 However, before clinical implementation of such models can become a widespread reality, it is critical to address the underrepresentation of many ethnic groups and the social, environmental, and health disparities prevalent in clinical research and healthcare datasets.16,17 Machine learning algorithms may exacerbate inherent biases in the training data, leading to biased findings and contravening the fundamental biomedical ethical principle of “do no harm” (e.g., through falsely finding differences in the level of urgent care needed among equally sick patients from different ethnic groups).18,19 Additionally, clinically underserved communities are unlikely to develop trust in machine-learning-guided genomic-based treatment plans unless health disparities research is incorporated from the start of the model-building process. To engender trust in the use of these approaches and to build a culture of ethical and transparent machine-learning applications in genomic medicine, partnerships should be promoted among the full spectrum of stakeholders. This includes clinical research participants, genomics and ELSI researchers, machine-learning scientists, and advocates for the clinical populations being studied (e.g., rare disease community organizers). As an example of this promoted partnership model, machine-learning model developers working in clinical settings could be required to develop an understanding of health disparities research as a prerequisite for applying their models to patient data. Such partnerships could also alleviate the concern that machine-learning algorithms used in genomic medicine may reduce the role of physicians, which may be attributed to clinicians and machine-learning scientists operating in siloed environments.20,21\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVs60sNUR9Wr",
        "outputId": "d1c3c07a-0856-4848-9c1d-d096ef38cb7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('clinical', 0.0087), ('research', 0.0087), ('disparity', 0.0087), ('model', 0.0087), ('patient', 0.0086)]\n",
            "Topic 2: [('model', 0.0271), ('clinical', 0.0271), ('research', 0.022), ('health', 0.0169), ('may', 0.0169)]\n",
            "Topic 3: [('model', 0.0087), ('research', 0.0087), ('clinical', 0.0087), ('genomic', 0.0086), ('medicine', 0.0086)]\n",
            "\n",
            "Key Phrases:\n",
            "- machine - learning - guided genomic - based treatment plans (score: 0.0328)\n",
            "- social , environmental , health disparities (score: 0.0262)\n",
            "- ethical transparent machine - learning applications (score: 0.023)\n",
            "- ( e.g. , rare disease community organizers (score: 0.023)\n",
            "- patient ’s genetic makeup.15 (score: 0.0164)\n",
            "\n",
            "Summary:\n",
            "Machine learning models have several applications in the implementation of genomic medicine such as to recommend diagnostic tools and pharmacogenomic therapies based on the patient’s genetic makeup.15 However, before clinical implementation of such models can become a widespread reality, it is critical to address the underrepresentation of many ethnic groups and the social, environmental, and health disparities prevalent in clinical research and healthcare datasets.16,17 Machine learning algorithms may exacerbate inherent biases in the training data, leading to biased findings and contravening the fundamental biomedical ethical principle of “do no harm” (e.g., through falsely finding differences in the level of urgent care needed among equally sick patients from different ethnic groups).18,19 Additionally, clinically underserved communities are unlikely to develop trust in machine-learning-guided genomic-based treatment plans unless health disparities research is incorporated from the start of the model-building process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"Concurrently with inclusion of underrepresented minorities, other ELSI considerations include establishing standards and/or guiding principles for explainability, transparency, reproducibility, trustworthiness, and accountability regarding machine learning applications in genomic medicine.22 At present, ELSI research at the interface of machine learning and genomic medicine reveals a multitude of scenarios in need of further research support or regulatory clarification. For example, when treatment plans for individual patients involve input from machine learning algorithms, how to assign accountability for adverse clinical outcomes among healthcare practitioners, regulatory bodies, and algorithm developers is currently unclear.23 The development of tools such as feature attribution methods that measure the impact of each feature on a certain prediction should be promoted, as well as algorithmic impact assessment frameworks to promote transparency and accountability.24,25 Ultimately, the application of machine learning approaches in the genomic medicine and healthcare delivery setting will require partnerships and collaborations with US domestic and international regulatory agencies, such as the US Food and Drug Administration and its worldwide counterparts.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLVp_kvTR-DG",
        "outputId": "9f6cb9ff-29b6-4660-ebe7-9f0a0bdec2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('machine', 0.0336), ('learning', 0.0336), ('regulatory', 0.0258), ('genomic', 0.0258), ('algorithm', 0.0181)]\n",
            "Topic 2: [('learning', 0.0124), ('machine', 0.0124), ('genomic', 0.0124), ('regulatory', 0.0124), ('research', 0.0124)]\n",
            "Topic 3: [('learning', 0.0124), ('machine', 0.0124), ('regulatory', 0.0124), ('genomic', 0.0124), ('impact', 0.0124)]\n",
            "\n",
            "Key Phrases:\n",
            "- genomic medicine healthcare delivery setting (score: 0.0393)\n",
            "- US domestic international regulatory agencies (score: 0.0337)\n",
            "- US Food Drug Administration (score: 0.0337)\n",
            "- ELSI considerations (score: 0.0169)\n",
            "- machine learning applications (score: 0.0169)\n",
            "\n",
            "Summary:\n",
            "Concurrently with inclusion of underrepresented minorities, other ELSI considerations include establishing standards and/or guiding principles for explainability, transparency, reproducibility, trustworthiness, and accountability regarding machine learning applications in genomic medicine.22 At present, ELSI research at the interface of machine learning and genomic medicine reveals a multitude of scenarios in need of further research support or regulatory clarification. For example, when treatment plans for individual patients involve input from machine learning algorithms, how to assign accountability for adverse clinical outcomes among healthcare practitioners, regulatory bodies, and algorithm developers is currently unclear.23 The development of tools such as feature attribution methods that measure the impact of each feature on a certain prediction should be promoted, as well as algorithmic impact assessment frameworks to promote transparency and accountability.24,25 Ultimately, the application of machine learning approaches in the genomic medicine and healthcare delivery setting will require partnerships and collaborations with US domestic and international regulatory agencies, such as the US Food and Drug Administration and its worldwide counterparts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"The rapid expansion of genomics and its applications in precision medicine, together with the current surge of machine learning usage in biomedical research, should be put on a sustainable track by adequate investment in multidisciplinary workforce development, ideally targeting college-level students as a future resource in addition to doctoral students and postdoctoral fellows. Current training programs in genomics and machine learning are typically compartmentalized, as trainees usually have either experimental or computational exposure to genomics research, and, conversely, computer science and machine learning students usually have minimal genomics training. To be able to interpret massive and multidimensional datasets, genomics researchers should be introduced to the fundamentals of machine learning early in their career path. Vice versa, genomics offers machine learning practitioners opportunities to solve fundamental questions in biology and medicine, and hence corresponding efforts should be made to increase knowledge of genomics fundamentals among that community.26 In addition, partnerships are encouraged between academic genomics research institutions and private industries that competitively recruit machine learning scientists with significant remuneration and benefits. A possible route to such synergies could be through establishing non-traditional scientist positions in academic research institutions that offer competitive salaries funded by industry for research projects that are of interest to both the academic and corporate stakeholders.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKr1OMhtR-tJ",
        "outputId": "07a0f5f7-026c-4476-b525-8024a6347bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('genomics', 0.0106), ('learning', 0.0105), ('machine', 0.0105), ('research', 0.0105), ('student', 0.0105)]\n",
            "Topic 2: [('genomics', 0.0105), ('machine', 0.0105), ('research', 0.0105), ('learning', 0.0104), ('fundamental', 0.0104)]\n",
            "Topic 3: [('genomics', 0.0508), ('learning', 0.0386), ('machine', 0.0386), ('research', 0.0325), ('academic', 0.0203)]\n",
            "\n",
            "Key Phrases:\n",
            "- academic corporate stakeholders (score: 0.0263)\n",
            "- either experimental computational exposure (score: 0.0219)\n",
            "- non - traditional scientist positions (score: 0.0219)\n",
            "- college - level students (score: 0.0175)\n",
            "- massive multidimensional datasets (score: 0.0175)\n",
            "\n",
            "Summary:\n",
            "Vice versa, genomics offers machine learning practitioners opportunities to solve fundamental questions in biology and medicine, and hence corresponding efforts should be made to increase knowledge of genomics fundamentals among that community.26 In addition, partnerships are encouraged between academic genomics research institutions and private industries that competitively recruit machine learning scientists with significant remuneration and benefits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"In this perspective, we identify key opportunities and challenges and set priorities for future activities in support of the adoption of machine learning approaches in genomics research. Not just at the NHGRI but also at NIH in general, promoting the convergence of machine learning and biomedicine is viewed as a high priority. In Figure 1, we have summarized the key challenge areas for this convergence as viewed through a genomics-focused lens and plan to address them in the near future, while also leveraging recent progress made by related activities at the NIH such as the Bridge2AI and AIM-AHEAD programs or the European Union’s GenoMed4All project (Table 1). Indeed, the Bridge2AI program, which has been designed from the ground up for artificial and human intelligence to work in hand, has a substantial genomics component through the CM4AI data generation project, which seeks to map the spatiotemporal architecture of human cells and use these maps toward the grand challenge of interpretable genotype-phenotype learning (Table 1). However, at this time, the gap between the state of current datasets and the needs of the field remains massive. While not all ideas and gaps discussed in this perspective may get addressed by future NHGRI- and NIH-supported research programs, through a combination of community input solicited by the NHGRI and detailed analysis of existing NIH funding portfolios in relevant areas, we expect to develop an evidence-based strategy for the NHGRI to support the convergence of machine learning and genomics.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FGlv4kJR_aS",
        "outputId": "226c93b5-834d-401b-c11f-5e5427389af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('learning', 0.0108), ('program', 0.0108), ('machine', 0.0108), ('convergence', 0.0108), ('genomics', 0.0108)]\n",
            "Topic 2: [('learning', 0.0108), ('genomics', 0.0108), ('challenge', 0.0108), ('future', 0.0108), ('convergence', 0.0108)]\n",
            "Topic 3: [('learning', 0.0273), ('nhgri', 0.021), ('nih', 0.021), ('1', 0.021), ('machine', 0.021)]\n",
            "\n",
            "Key Phrases:\n",
            "- future NHGRI- NIH - supported research programs (score: 0.029)\n",
            "- Bridge2AI AIM - AHEAD programs (score: 0.0254)\n",
            "- European Union ’s GenoMed4All project (score: 0.0217)\n",
            "- genomics - focused lens (score: 0.0181)\n",
            "- CM4AI data generation project (score: 0.0181)\n",
            "\n",
            "Summary:\n",
            "Indeed, the Bridge2AI program, which has been designed from the ground up for artificial and human intelligence to work in hand, has a substantial genomics component through the CM4AI data generation project, which seeks to map the spatiotemporal architecture of human cells and use these maps toward the grand challenge of interpretable genotype-phenotype learning (Table 1). While not all ideas and gaps discussed in this perspective may get addressed by future NHGRI- and NIH-supported research programs, through a combination of community input solicited by the NHGRI and detailed analysis of existing NIH funding portfolios in relevant areas, we expect to develop an evidence-based strategy for the NHGRI to support the convergence of machine learning and genomics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"Your paragraph text here\")"
      ],
      "metadata": {
        "id": "jUevMwdbSAyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"Machine learning approaches in genomics and other biomedical research fields depend on comprehensive and systematic FAIRness of training datasets (Table 1)6; hence, early and easy access to both raw and processed datasets coming out of genomics research networks and consortia should be promoted. As an extension of this theme, more DNA sequencing data need to be generated from across different branches of the evolutionary tree to enable the development of models that use evolutionary and information theory principles. In addition, the development of machine learning approaches that leverage and integrate multiple data types (e.g., population genomics, functional genomics, and single-cell genome-wide imaging data) to generate biological insights is also a high priority. While data exist from hundreds of thousands of genome-wide association study (GWAS) samples, only a few thousand have expression quantitative trait locus (eQTL) data available,7 and available gene expression data are very limited by the specific biological context from which they were collected (being mostly from adult tissue cell lines). Ideally, a variety of data types (epigenetic, expression, and genome sequencing) derived from different cell types, sample collection modalities, and sampled populations should be accessible to machine-learning methods developers. Training datasets should also be augmented with data derived from statistically designed, model-driven experiments, including perturbation assays.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYtenGngB8Xx",
        "outputId": "5dfb4faf-d3cb-4bb1-924e-b9e024958deb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('data', 0.0508), ('genomics', 0.0264), ('type', 0.0203), ('expression', 0.0203), ('datasets', 0.0203)]\n",
            "Topic 2: [('data', 0.0102), ('genomics', 0.0101), ('expression', 0.0101), ('different', 0.0101), ('derived', 0.0101)]\n",
            "Topic 3: [('data', 0.0102), ('genomics', 0.0101), ('datasets', 0.0101), ('type', 0.0101), ('expression', 0.0101)]\n",
            "\n",
            "Key Phrases:\n",
            "- genome - wide association study ( GWAS ) samples (score: 0.036)\n",
            "- single - cell genome - wide imaging data (score: 0.032)\n",
            "- statistically designed , model - driven experiments (score: 0.028)\n",
            "- hence , early easy access (score: 0.024)\n",
            "- available,7 available gene expression data (score: 0.024)\n",
            "\n",
            "Summary:\n",
            "In addition, the development of machine learning approaches that leverage and integrate multiple data types (e.g., population genomics, functional genomics, and single-cell genome-wide imaging data) to generate biological insights is also a high priority. While data exist from hundreds of thousands of genome-wide association study (GWAS) samples, only a few thousand have expression quantitative trait locus (eQTL) data available,7 and available gene expression data are very limited by the specific biological context from which they were collected (being mostly from adult tissue cell lines).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"Also of critical importance is the availability of experimental metadata annotation, with such shared genomics metadata optimized for machine learning approaches (including not only the sample descriptions in a structured standardized format but also quality control parameters). In this context, best practices for robust machine-learning-amenable dataset generation with extensive, standardized metadata should be developed, including ways to annotate perturbation datasets. Current strategies for releasing processed large-scale genomics data are geared toward formats designed for genome browsers or sequence data analysis pipelines rather than as input for machine learning models. This places an additional burden on users who would like to instead apply machine learning-based analytics to the datasets. As an example of good practices in this area, the NIH Common Fund Epigenomics Program (Table 1) was highlighted. Through the combined efforts of federal science administrators and awardees who were part of the program, a strong and consistent focus was maintained throughout the program life cycle on creating data that were made accessible from an early stage (almost 2 years before their initial paper was published) and were accompanied by consistently formatted and contextually deep metadata files derived from a consortium-wide “matrix of experiments.”8,9 Alongside the raw data and quality control metrics, the availability of such metadata allowed the final dataset to become a substrate for development of multiple machine-learning-based epigenome analysis tools.10,11\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmkXU55gLWgo",
        "outputId": "9519e605-c7db-4c0d-805e-8c617d41828f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('data', 0.009), ('metadata', 0.009), ('program', 0.009), ('machine', 0.009), ('control', 0.009)]\n",
            "Topic 2: [('metadata', 0.0308), ('data', 0.025), ('program', 0.0192), ('machine', 0.0192), ('availability', 0.0135)]\n",
            "Topic 3: [('metadata', 0.009), ('machine', 0.009), ('data', 0.0089), ('program', 0.0089), ('including', 0.0089)]\n",
            "\n",
            "Key Phrases:\n",
            "- robust machine - learning - amenable dataset generation (score: 0.0311)\n",
            "- multiple machine - learning - based epigenome analysis (score: 0.0311)\n",
            "- consistently formatted contextually deep metadata files (score: 0.0272)\n",
            "- processed large - scale genomics data (score: 0.0233)\n",
            "- NIH Common Fund Epigenomics Program (score: 0.0233)\n",
            "\n",
            "Summary:\n",
            "Also of critical importance is the availability of experimental metadata annotation, with such shared genomics metadata optimized for machine learning approaches (including not only the sample descriptions in a structured standardized format but also quality control parameters). Through the combined efforts of federal science administrators and awardees who were part of the program, a strong and consistent focus was maintained throughout the program life cycle on creating data that were made accessible from an early stage (almost 2 years before their initial paper was published) and were accompanied by consistently formatted and contextually deep metadata files derived from a consortium-wide “matrix of experiments.”8,9 Alongside the raw data and quality control metrics, the availability of such metadata allowed the final dataset to become a substrate for development of multiple machine-learning-based epigenome analysis tools.10,11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"As they stand, the majority of machine learning algorithms used in genomics are typically developed for other research fields and retrospectively optimized for genomics research. An increased emphasis is needed on machine learning models that are built specifically with the challenges facing genomics being kept in mind that are not only predictive but can also be used to infer causality from genomic changes. To facilitate this, testing of functional biology insights derived from machine learning applications deployed on existing observational datasets would provide a greater experimental knowledge base for machine-learning-based causal modeling in genomics. This is particularly relevant, for example, when studying how genetic variation associates with phenotype and gene function, such as to understand the genomic architecture of complex disease phenotypes, and how variants impact gene expression. For complex disease phenotypes, while linear regression models may be used as a baseline, more complex non-linear methods like neural networks and random forests may yield new insights when there is evidence of non-linearity (especially when large training datasets are available). For example, large-scale biobank programs, such as the US-based All of Us Research Program, the UK Biobank, or national networks such as the Australian Genomics-supported Program in Advanced Genomic Investigation (PAGI) (Table 1), are fertile grounds for machine learning in genomic medicine, especially due to the comprehensive, high-quality, and multimodal data that are being collected in such projects. Applying machine learning in such contexts could elucidate the role of genotype-phenotype-environment interactions and the genetic effects shared across traits.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46Ucz6wDLW1V",
        "outputId": "e8ce059b-5538-4333-d7cf-4526ce81d4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('learning', 0.0091), ('genomic', 0.009), ('genomics', 0.009), ('phenotype', 0.009), ('machine', 0.009)]\n",
            "Topic 2: [('learning', 0.0091), ('machine', 0.0091), ('genomics', 0.0091), ('genomic', 0.009), ('research', 0.009)]\n",
            "Topic 3: [('machine', 0.0291), ('learning', 0.0291), ('genomic', 0.0237), ('genomics', 0.0237), ('complex', 0.0182)]\n",
            "\n",
            "Key Phrases:\n",
            "- comprehensive , high - quality , multimodal data (score: 0.0338)\n",
            "- machine - learning - based causal modeling (score: 0.0236)\n",
            "- , complex non - linear methods (score: 0.0236)\n",
            "- Australian Genomics - supported Program (score: 0.0203)\n",
            "- genotype - phenotype - environment interactions (score: 0.0203)\n",
            "\n",
            "Summary:\n",
            "For complex disease phenotypes, while linear regression models may be used as a baseline, more complex non-linear methods like neural networks and random forests may yield new insights when there is evidence of non-linearity (especially when large training datasets are available). For example, large-scale biobank programs, such as the US-based All of Us Research Program, the UK Biobank, or national networks such as the Australian Genomics-supported Program in Advanced Genomic Investigation (PAGI) (Table 1), are fertile grounds for machine learning in genomic medicine, especially due to the comprehensive, high-quality, and multimodal data that are being collected in such projects.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"The accuracy and performance of most supervised machine learning models are inherently linked to the availability of suitably large training datasets; however, existing large and machine learning-amenable datasets are few and far between in genomics. Therefore, newer and less data-hungry methods that can still yield rich mechanistic and causal models in genomics are needed. Methods such as zero-shot learning, where the model is able to learn even in contexts not observed during training, may have applications in genomic medicine.12 As an example, a model originally trained on cell culture data could be subsequently used on patient-derived xenograft data and eventually guide patient treatment plans. Generative adversarial networks (GANs) and adversarial training generate “realistic” simulated data to train machine learning methods by combining small amounts of biologically observed data with simulated data.13 This creates a larger training dataset that is expected to have the same characteristics as the original dataset. However, the imbalance between effectively unlimited simulated data versus limited observed biological data could result in machine-learning models that more closely represent the idiosyncrasies of the simulator than the actual biology of the studied system. Given that for some studies, such as evolutionary genetics and ancestral population studies or forensic DNA analyses, high-quality biological samples for genomic studies are limited, it is critical to delineate the circumstances in which synthetic training datasets are useful and, when they are, how such data can be generated to be representative of actual biological or population data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gbbE4JwLXt2",
        "outputId": "d6e42aa5-5408-4649-8f1c-d3dd41128e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('data', 0.0101), ('model', 0.0101), ('training', 0.01), ('learning', 0.01), ('study', 0.01)]\n",
            "Topic 2: [('data', 0.0478), ('training', 0.0306), ('model', 0.0306), ('method', 0.0191), ('machine', 0.0191)]\n",
            "Topic 3: [('data', 0.0101), ('model', 0.0101), ('training', 0.01), ('biological', 0.01), ('observed', 0.01)]\n",
            "\n",
            "Key Phrases:\n",
            "- existing large machine learning - amenable datasets (score: 0.029)\n",
            "- newer less data - hungry methods (score: 0.0254)\n",
            "- supervised machine learning models (score: 0.0181)\n",
            "- rich mechanistic causal models (score: 0.0181)\n",
            "- patient - derived xenograft data (score: 0.0181)\n",
            "\n",
            "Summary:\n",
            "Methods such as zero-shot learning, where the model is able to learn even in contexts not observed during training, may have applications in genomic medicine.12 As an example, a model originally trained on cell culture data could be subsequently used on patient-derived xenograft data and eventually guide patient treatment plans. Generative adversarial networks (GANs) and adversarial training generate “realistic” simulated data to train machine learning methods by combining small amounts of biologically observed data with simulated data.13 This creates a larger training dataset that is expected to have the same characteristics as the original dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"To maximize the availability of suitable training datasets, which may currently be compartmentalized in different data repositories under different institutional data governance and access policies, and national and international data access regulations (e.g., in the Data Science for Health Discovery and Innovation in Africa [DS-I Africa] program) (Table 1), federated data infrastructure for genomics is a crucial need. Specifically, federated data technology enables virtual unification of data from different sources under a uniform data model, while the underlying data stores operate autonomously and without data leaving their original locations. This would allow for a larger number of currently isolated genomics training datasets to become accessible to machine learning models, which could run federated queries as though the data were combined. Together with such federated data infrastructures, privacy-preserving technologies enabling safe and ethical data access should be pursued.14\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQxwc5XeLYNF",
        "outputId": "aea5511a-cc5c-472c-d459-25c8219d4b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('data', 0.1187), ('federated', 0.0386), ('different', 0.0297), ('access', 0.0297), ('datasets', 0.0208)]\n",
            "Topic 2: [('data', 0.0159), ('federated', 0.0157), ('different', 0.0157), ('model', 0.0157), ('access', 0.0156)]\n",
            "Topic 3: [('data', 0.0165), ('access', 0.0158), ('federated', 0.0158), ('different', 0.0157), ('infrastructure', 0.0157)]\n",
            "\n",
            "Key Phrases:\n",
            "- national international data access regulations (score: 0.038)\n",
            "- currently isolated genomics training datasets (score: 0.0316)\n",
            "- safe ethical data access (score: 0.0316)\n",
            "- different institutional data governance (score: 0.0253)\n",
            "- uniform data model (score: 0.0253)\n",
            "\n",
            "Summary:\n",
            "To maximize the availability of suitable training datasets, which may currently be compartmentalized in different data repositories under different institutional data governance and access policies, and national and international data access regulations (e.g., in the Data Science for Health Discovery and Innovation in Africa [DS-I Africa] program) (Table 1), federated data infrastructure for genomics is a crucial need.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"In this context, the National Human Genome Research Institute (NHGRI), which spearheads genomics research at the US National Institutes of Health,5 seeks to help define a path forward for allowing machine learning to be used productively in genomics research. While the examples above represent some ways in which machine learning has already organically contributed to genomics research, the anticipated acceleration of developments and advances motivated the NHGRI to bring members of the genomics and machine learning communities together with bioethics researchers and social scientists to create a roadmap for convergence of these three fields in an ethical, transparent, and equitable manner. This convening role was approached through multiple routes, such as conversations with NHGRI awardees in these areas as part of the institute’s strategic planning process (Table 1)5 and hosting events such as the NHGRI’s 2021 “Machine Learning in Genomics: Tools, Resources, Clinical Applications, and Ethics” workshop. Cumulatively, these community engagement exercises served to identify opportunities and obstacles underlying the application of machine learning methods to basic genome sciences and genomic medicine, to define the key scientific areas in genomics that could benefit from machine learning analyses, and to build a map for the NHGRI’s unique role in pursuing those efforts. Several challenges facing the convergence of machine learning and genomics research and a set of recommendations were identified for the NHGRI to consider in developing its strategic priorities in this area (briefly described in the following section and summarized in Figure 1).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0i-GLNnAwzv",
        "outputId": "c73d26d4-3e4e-4b39-9505-886b43a7b20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('machine', 0.0093), ('genomics', 0.0093), ('learning', 0.0093), ('nhgri', 0.0093), ('research', 0.0093)]\n",
            "Topic 2: [('learning', 0.0399), ('genomics', 0.0399), ('machine', 0.0399), ('nhgri', 0.0344), ('research', 0.029)]\n",
            "Topic 3: [('genomics', 0.0093), ('learning', 0.0093), ('machine', 0.0093), ('nhgri', 0.0093), ('research', 0.0093)]\n",
            "\n",
            "Key Phrases:\n",
            "- ethical , transparent , equitable manner (score: 0.0295)\n",
            "- NHGRI ’s 2021 “ Machine Learning (score: 0.0258)\n",
            "- National Human Genome Research Institute (score: 0.0221)\n",
            "- institute ’s strategic planning process (score: 0.0221)\n",
            "- NHGRI ’s unique role (score: 0.0185)\n",
            "\n",
            "Summary:\n",
            "Cumulatively, these community engagement exercises served to identify opportunities and obstacles underlying the application of machine learning methods to basic genome sciences and genomic medicine, to define the key scientific areas in genomics that could benefit from machine learning analyses, and to build a map for the NHGRI’s unique role in pursuing those efforts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"Ever since early uses of machine learning in genomics (e.g., for defining protein-coding sequences in Sanger sequencing data) through to the current era of massively parallel DNA sequencing, machine learning has consistently been a versatile tool for annotating genomes and extracting knowledge from raw DNA sequence data.3 Diverse applications of machine learning in genomics include genome sequence assembly, gene identification, annotation of gene function, genomic variant calling, modeling of sequence evolution, genome-wide association and genotype-phenotype predictions, inferring gene interactions, and many more.4 This list of applications grows much larger when considering the other omics sciences, such as transcriptomics, proteomics, metabolomics, and metagenomics. Particularly in the last decade, through a combination of accelerated basic research in artificial intelligence coupled with advances in computational hardware, the application of machine learning to biomedical research questions has seen a sharp acceleration. This surge is not unique to genomics and has occurred with many biomedical research fields (e.g., those leveraging imaging data and electronic health records); however, the application of machine learning to each of these data types presents unique challenges that need to be addressed to empower the next phase of biomedical machine learning research. Such challenges include both domain-specific technical hurdles (e.g., developing standards for artificial intelligence “readiness” that are optimized for individual data types) as well as challenges that are common across all fields of biomedicine (e.g., the requirement for transparency and interpretability in machine learning algorithms, defined as the ability for humans to understand and be able to explain, in human terms, the decisions or predictions made by these algorithms).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W44DmscLXhzQ",
        "outputId": "0edf0384-a3d1-4a12-b2df-01d0a3879985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('learning', 0.0372), ('machine', 0.0372), ('sequence', 0.022), ('data', 0.022), ('research', 0.022)]\n",
            "Topic 2: [('machine', 0.0089), ('learning', 0.0089), ('application', 0.0088), ('biomedical', 0.0088), ('data', 0.0088)]\n",
            "Topic 3: [('learning', 0.0088), ('machine', 0.0088), ('research', 0.0088), ('application', 0.0088), ('data', 0.0088)]\n",
            "\n",
            "Key Phrases:\n",
            "- domain - specific technical hurdles (score: 0.0197)\n",
            "- ( e.g. , developing standards (score: 0.0164)\n",
            "- protein - coding sequences (score: 0.0132)\n",
            "- genome - wide association (score: 0.0132)\n",
            "- genotype - phenotype predictions (score: 0.0132)\n",
            "\n",
            "Summary:\n",
            "Ever since early uses of machine learning in genomics (e.g., for defining protein-coding sequences in Sanger sequencing data) through to the current era of massively parallel DNA sequencing, machine learning has consistently been a versatile tool for annotating genomes and extracting knowledge from raw DNA sequence data.3 Diverse applications of machine learning in genomics include genome sequence assembly, gene identification, annotation of gene function, genomic variant calling, modeling of sequence evolution, genome-wide association and genotype-phenotype predictions, inferring gene interactions, and many more.4 This list of applications grows much larger when considering the other omics sciences, such as transcriptomics, proteomics, metabolomics, and metagenomics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"The data-intensive fields of genomics and machine learning (ML) are in an early stage of convergence. Genomics researchers increasingly seek to harness the power of ML methods to extract knowledge from their data; conversely, ML scientists recognize that genomics offers a wealth of large, complex, and well-annotated datasets that can be used as a substrate for developing biologically relevant algorithms and applications. The National Human Genome Research Institute (NHGRI) inquired with researchers working in these two fields to identify common challenges and receive recommendations to better support genomic research efforts using ML approaches. Those included increasing the amount and variety of training datasets by integrating genomic with multiomics, context-specific (e.g., by cell type), and social determinants of health datasets; reducing the inherent biases of training datasets; prioritizing transparency and interpretability of ML methods; and developing privacy-preserving technologies for research participants’ data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvXd40yeQ_WD",
        "outputId": "2691269c-988c-4751-ae34-c78d265e6113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('ml', 0.0468), ('datasets', 0.038), ('genomics', 0.0292), ('research', 0.0292), ('training', 0.0205)]\n",
            "Topic 2: [('ml', 0.014), ('datasets', 0.0139), ('research', 0.0139), ('genomics', 0.0139), ('genomic', 0.0139)]\n",
            "Topic 3: [('ml', 0.0139), ('research', 0.0139), ('genomics', 0.0139), ('datasets', 0.0139), ('developing', 0.0139)]\n",
            "\n",
            "Key Phrases:\n",
            "- large , complex , well - annotated datasets (score: 0.0529)\n",
            "- National Human Genome Research Institute (score: 0.0353)\n",
            "- data - intensive fields (score: 0.0294)\n",
            "- privacy - preserving technologies (score: 0.0235)\n",
            "- research participants ’ data (score: 0.0235)\n",
            "\n",
            "Summary:\n",
            "Those included increasing the amount and variety of training datasets by integrating genomic with multiomics, context-specific (e.g., by cell type), and social determinants of health datasets; reducing the inherent biases of training datasets; prioritizing transparency and interpretability of ML methods; and developing privacy-preserving technologies for research participants’ data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_new_paragraph(\"Artificial intelligence is the science and engineering of making intelligent machines, especially intelligent computer programs.1 Within the broader field of artificial intelligence, machine learning is the study of computer algorithms that improve automatically through experience.2 Genomics and machine learning have a shared history dating back nearly a quarter century, with the first applications of machine learning methods on DNA sequence data being reported soon after the beginning of the Human Genome Project. Nowadays, genomics is inherently a data-intensive field of research; in fact, since the advent of next-generation DNA-sequencing methods, truly massive volumes of exome, genome, and transcriptome sequencing data have been generated, often with rich and complex metadata annotations. This rich data landscape, which includes not just sequencing data but additional layers of information such as functional genomics and single-cell profiling, provides a natural resource for the use of machine learning to derive biologically and clinically meaningful insights.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXCUN1rzXiwE",
        "outputId": "fa6cfd37-c412-4a9b-c19c-b6a2295d053c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics:\n",
            "Topic 1: [('machine', 0.0461), ('data', 0.0375), ('learning', 0.0375), ('genomics', 0.0288), ('genome', 0.0202)]\n",
            "Topic 2: [('machine', 0.0141), ('learning', 0.0141), ('genomics', 0.0141), ('data', 0.0141), ('rich', 0.0141)]\n",
            "Topic 3: [('data', 0.0142), ('machine', 0.0142), ('learning', 0.0141), ('genomics', 0.0141), ('computer', 0.0141)]\n",
            "\n",
            "Key Phrases:\n",
            "- exome , genome , transcriptome sequencing data (score: 0.0465)\n",
            "- next - generation DNA - sequencing methods (score: 0.0407)\n",
            "- data - intensive field (score: 0.0291)\n",
            "- rich complex metadata annotations (score: 0.0291)\n",
            "- especially intelligent computer programs.1 (score: 0.0233)\n",
            "\n",
            "Summary:\n",
            "Artificial intelligence is the science and engineering of making intelligent machines, especially intelligent computer programs.1 Within the broader field of artificial intelligence, machine learning is the study of computer algorithms that improve automatically through experience.2 Genomics and machine learning have a shared history dating back nearly a quarter century, with the first applications of machine learning methods on DNA sequence data being reported soon after the beginning of the Human Genome Project.\n"
          ]
        }
      ]
    }
  ]
}